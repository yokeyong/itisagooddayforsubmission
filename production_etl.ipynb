{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "486928c3",
   "metadata": {},
   "source": [
    "## HDB's Technical Test for Senior Data Engineer\n",
    "#### Name: Wong Yoke Yong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d55f6c6",
   "metadata": {},
   "source": [
    "Instruction:\n",
    "\n",
    "1. Click on \"Run All\" button."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b8cfd6",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d2dbaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic EDA\n",
    "from typing import List, Any, Tuple\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Download the dataset from the provided URL\n",
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "## Data Profiling\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "## Data Validation\n",
    "import great_expectations as gx\n",
    "from great_expectations import ResultFormat\n",
    "from datetime import datetime\n",
    "\n",
    "## Hashing\n",
    "import hashlib\n",
    "\n",
    "## Logging\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d9a476",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597e73d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://data.gov.sg\"\n",
    "BATCH_URL_SIZE = 10\n",
    "NEEDED_FILES = ['Resale Flat Prices (Based on Registration Date), From Mar 2012 to Dec 2014.csv', \n",
    "                'Resale Flat Prices (Based on Registration Date), From Jan 2015 to Dec 2016.csv', \n",
    "                'Resale Flat Prices (Based on Approval Date), 2000 - Feb 2012.csv']\n",
    "ALL_FILES = list(os.listdir())\n",
    "HEADERS = {\"x-api-key\": <YOUR_API_KEY_HERE>}\n",
    "VALIDATION_RESULTS_SUCCESS = \"validation_success_summary.txt\"\n",
    "VALIDATION_RESULTS_FAILED = \"validation_failed_summary.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ef7312",
   "metadata": {},
   "source": [
    "## Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aff7ae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(dataset: pd.DataFrame, filename: str) -> None:\n",
    "    \"\"\"Writes the provided DataFrame to a CSV file.\"\"\"\n",
    "    dataset.to_csv(filename, index=False)\n",
    "    logger.info(f\"Successfully wrote DataFrame to CSV file: {filename}\")\n",
    "\n",
    "def write_to_text_file_failure_records_and_retrieve_index(validation_results: dict) -> list:\n",
    "    \"\"\"Write the validation records to the text file for checking purposes.\n",
    "    This is for failed cases.\"\"\"\n",
    "    with open(VALIDATION_RESULTS_FAILED, \"w\") as f:\n",
    "        f.write(f\"Checkpoint Status: {validation_results.get('success')}\\n\")\n",
    "        output_list = []\n",
    "        results_list = validation_results.get(\"results\")\n",
    "        for result in results_list:\n",
    "            if not result.get(\"success\"):\n",
    "                f.write(f\"FAILED: {result.get('expectation_config').get('type')}\\n\") \n",
    "                f.write(f\"Observed: {result.get('result').get('observed_value')}\\n\\n\")\n",
    "                f.write(str(result.to_json_dict()))\n",
    "                output_list.append(result.to_json_dict())\n",
    "        return output_list\n",
    "    logger.info(\"Written the failed validation definition to a text file for checking purposes\")\n",
    "\n",
    "def write_to_text_file_success_records(validation_results: dict) -> None:\n",
    "    \"\"\"Write the validation records to the text file for checking purposes. \n",
    "    This is for successful cases.\"\"\"\n",
    "    with open(VALIDATION_RESULTS_SUCCESS, \"w\") as f:\n",
    "        f.write(f\"Checkpoint Status: {validation_results.get('success')}\\n\")\n",
    "        results_list = validation_results.get(\"results\")\n",
    "        for result in results_list:\n",
    "            if result.get(\"success\"):\n",
    "                f.write(f\"SUCCESS: {result.get('expectation_config').get('type')}\\n\") \n",
    "                f.write(f\"Observed: {result.get('result').get('observed_value')}\\n\\n\")\n",
    "    logger.info(\"Written the successful validation definition to a text file for checking purposes\")\n",
    "\n",
    "def remove_duplicates_based_on_fields(fields: list, dataset: pd.DataFrame) -> None:\n",
    "    \"\"\"Remove duplicates from the dataset based on the sorted fields.\"\"\"\n",
    "    logger.info(\"Remove duplicates based on the fields supplied for sorting first, then to remove duplicates.\")\n",
    "    try:\n",
    "        dataset = dataset.sort_values(by=fields, ascending=[False, False])\n",
    "        master_to_keep_unique_records = dataset[~dataset.duplicated(keep='first')]\n",
    "        master_failed_unique_records = dataset[dataset.duplicated(keep='first')]\n",
    "        return (master_to_keep_unique_records, master_failed_unique_records)\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Failed to remove duplicates from the dataset. Error - {e}\")\n",
    "        raise e\n",
    "\n",
    "def append_datasets_for_storage(datasets: list) -> pd.DataFrame:\n",
    "    \"\"\"General function for appending the datasets\"\"\"\n",
    "    try:\n",
    "        return pd.concat(datasets, axis=0)\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Issues encountered when appending raw datasets for storage. Error - {e}\")\n",
    "        raise e\n",
    "\n",
    "def filter_for_failed_validation_records(validation_failure_status: bool, output: list, dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Split the failed records from the successful records for ETL process\"\"\"\n",
    "    try:\n",
    "        failed_records_index = set()\n",
    "        if validation_failure_status:\n",
    "            for output_result in output:\n",
    "                failed_records_index = failed_records_index | set(output_result.get(\"result\").get('unexpected_index_list'))\n",
    "        failed_records_index_list = list(failed_records_index)\n",
    "        failed_records = dataset[dataset.index.isin(failed_records_index_list)]\n",
    "        success_records = dataset[~dataset.index.isin(failed_records_index_list)]\n",
    "        return (success_records, failed_records)\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Filter for failed validation records. Error - {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0503db8",
   "metadata": {},
   "source": [
    "## Source (Download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2ec060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selected_dataset() -> List[dict]:\n",
    "    \"\"\"Define the selected datasets.\"\"\"\n",
    "    selected_ids = [{0: (\"d_43f493c6c50d54243cc1eab0df142d6a\", \"Resale Flat Prices (Based on Approval Date), 2000 - Feb 2012.csv\")},\n",
    "                    {1: (\"d_2d5ff9ea31397b66239f245f57751537\", \"Resale Flat Prices (Based on Registration Date), From Mar 2012 to Dec 2014.csv\")},\n",
    "                    {2: (\"d_ea9ed51da2787afaf8e51f827c304208\", \"Resale Flat Prices (Based on Registration Date), From Jan 2015 to Dec 2016.csv\")}]\n",
    "    return selected_ids\n",
    "\n",
    "# Downloads the dataset from data.gov.sg\n",
    "def fetch_url(next_url: str) -> pd.DataFrame:\n",
    "    \"\"\"Fetches a single URL and returns its status code and URL.\"\"\"\n",
    "    try:\n",
    "        # Use the requests library to make a GET request\n",
    "        logger.info(f\"Fetching URL: {next_url}\")\n",
    "        response = requests.get(next_url, headers=HEADERS)\n",
    "        data = response.json()\n",
    "        column_names = [field[\"id\"] for field in data[\"result\"][\"fields\"]]\n",
    "        data_subset = pd.DataFrame(data[\"result\"][\"records\"], columns=column_names)\n",
    "        return data_subset\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.exception(f\"Error fetching URL: {next_url} - {e}\")\n",
    "        raise e\n",
    "\n",
    "# Main function for downloading dataset.\n",
    "def download_datasets(selected_ids: List[dict]) -> pd.DataFrame:\n",
    "    \"\"\"Downloads datasets from the provided list of selected dataset IDs from data.gov.sg.\"\"\"\n",
    "    try:\n",
    "\n",
    "        for index in range(len(selected_ids)):\n",
    "\n",
    "            # Define the starting URL for the current dataset\n",
    "            STARTING_URL = f\"{BASE_URL}/api/action/datastore_search?resource_id={selected_ids[index].get(index)[0]}\"\n",
    "\n",
    "            logger.info(f\"Starting download for dataset: {selected_ids[index].get(index)[1]}\")\n",
    "\n",
    "            # fetch the first page so we have a `data` object to follow the links\n",
    "            response = requests.get(STARTING_URL, headers=HEADERS)\n",
    "            data = response.json()\n",
    "            column_names = [f[\"id\"] for f in data[\"result\"][\"fields\"]]\n",
    "            first_dataset = pd.DataFrame(data[\"result\"][\"records\"], columns=column_names)\n",
    "\n",
    "            # retrieve the total number of records for logging purposes\n",
    "            total_records = data[\"result\"][\"total\"]\n",
    "            logger.info(f\"Total records to download for {selected_ids[index].get(index)[0]}: {total_records}\")\n",
    "\n",
    "            # master frame to accumulate all the pages\n",
    "            master_dataset = first_dataset.copy()\n",
    "\n",
    "            # walk through the remaining pages in chunks of ten urls\n",
    "            while \"next\" in data[\"result\"][\"_links\"]:\n",
    "                next_urls = []\n",
    "                # build list of up to ten `next` URLs\n",
    "                for _ in range(BATCH_URL_SIZE):\n",
    "                    next_link = data[\"result\"][\"_links\"].get(\"next\")\n",
    "                    if not next_link:\n",
    "                        break\n",
    "                    url = f\"{BASE_URL}{next_link}\"\n",
    "                    next_urls.append(url)\n",
    "                    # advance `data` so the following iteration of the inner loop\n",
    "                    # grabs the subsequent link\n",
    "                    resp = requests.get(url, headers=HEADERS)\n",
    "                    data = resp.json()\n",
    "\n",
    "                # download the batch concurrently\n",
    "                with concurrent.futures.ThreadPoolExecutor(max_workers=BATCH_URL_SIZE) as executor:\n",
    "                    dfs = list(executor.map(fetch_url, next_urls))\n",
    "\n",
    "                bigger_sub_dataset = pd.concat(dfs, ignore_index=True)\n",
    "                master_dataset = pd.concat([master_dataset, bigger_sub_dataset], ignore_index=True)\n",
    "                logger.info(f\"Number of rows and columns for Master Dataset: {master_dataset.shape}\")\n",
    "\n",
    "                if master_dataset.shape[0] == total_records:\n",
    "                    logger.info(f\"All records downloaded for {selected_ids[index].get(index)[1]}\")\n",
    "                    break\n",
    "\n",
    "                # pause for the rate limit\n",
    "                time.sleep(10)\n",
    "\n",
    "            # at the end `master_dataset` contains all of the concatenated records\n",
    "            write_to_csv(master_dataset, selected_dataset()[index].get(index)[1])\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error downloading datasets: {e}\")\n",
    "        raise e\n",
    "\n",
    "def source_download_main() -> None:\n",
    "    \"\"\"Main function to orchestrate the dataset download process.\"\"\"\n",
    "    logger.info(\"Starting the dataset download process.\")\n",
    "    selected_ids = selected_dataset()\n",
    "    download_datasets(selected_ids=selected_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7747ef10",
   "metadata": {},
   "source": [
    "## Ingestion (Extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e6fca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset() -> pd.DataFrame:\n",
    "    \"\"\"Read dataset from local files.\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Read in the three datasets that ranges from January 2000 to December 2016.\")\n",
    "        first = pd.read_csv(\"Resale Flat Prices (Based on Approval Date), 2000 - Feb 2012.csv\")\n",
    "        second = pd.read_csv(\"Resale Flat Prices (Based on Registration Date), From Mar 2012 to Dec 2014.csv\")\n",
    "        third = pd.read_csv(\"Resale Flat Prices (Based on Registration Date), From Jan 2015 to Dec 2016.csv\")\n",
    "        # Combines the three dataset.\n",
    "        master_dataset = pd.concat([first, second, third], ignore_index=True)\n",
    "        return master_dataset\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error reading datasets: {e}\")\n",
    "        raise e\n",
    "\n",
    "def dataset_type_enforcement(dataset: pd.DataFrame) -> None:\n",
    "    \"\"\"Enforce the correct data types for each column in the master dataset.\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Conduct type enforcement on the fields.\")\n",
    "        dataset.month = pd.to_datetime(dataset.month, format=\"%Y-%m\")\n",
    "        dataset.town = dataset.town.astype('category')\n",
    "        dataset.flat_type = dataset.flat_type.astype('category')\n",
    "        dataset.block = dataset.block.astype(str)\n",
    "        dataset.street_name = dataset.street_name.astype(str)\n",
    "        dataset.storey_range = dataset.storey_range.astype('category')\n",
    "        dataset.floor_area_sqm = dataset.floor_area_sqm.astype(float)\n",
    "        dataset.flat_model = dataset.flat_model.astype('category')\n",
    "        dataset.lease_commence_date = dataset.lease_commence_date.astype(int)\n",
    "        dataset.remaining_lease = dataset.remaining_lease.astype(float)\n",
    "        dataset._id = dataset._id.astype(int)\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error enforcing data types: {e}\")\n",
    "        raise e\n",
    "\n",
    "def filter_2012_to_2016(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Filter the master dataset to only include records from 2012 to 2016.\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Filter the dataset to contain between January 2012 to December 2016.\")\n",
    "        filtered_dataset = dataset[(dataset.month >= \"2012-01-01\") & (dataset.month <= \"2016-12-31\")]\n",
    "        return filtered_dataset\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error filtering dataset: {e}\")\n",
    "        raise e\n",
    "\n",
    "def load_up_dataset() -> pd.DataFrame:\n",
    "    \"\"\"Load up the dataset from the downloaded CSV files and concatenate\n",
    "    them into a dataset.\"\"\"\n",
    "    logger.info(\"Starting the dataset loading process.\")\n",
    "    full_raw_dataset = read_dataset()\n",
    "    logger.info(f\"Number of rows and columns for Full Raw Dataset: {full_raw_dataset.shape}\")\n",
    "    logger.info(\"Enforcing data types for the master dataset.\")\n",
    "    type_enforced_dataset = dataset_type_enforcement(dataset=full_raw_dataset)\n",
    "    logger.info(\"Filtering the master dataset to only include records from January 2012 to December 2016.\")\n",
    "    filtered_dataset = filter_2012_to_2016(dataset=type_enforced_dataset)\n",
    "    logger.info(f\"Number of rows and columns for Filtered Dataset: {filtered_dataset.shape}\")\n",
    "    return (full_raw_dataset, filtered_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb552b2",
   "metadata": {},
   "source": [
    "## Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ec48d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_profiling(dataset: pd.DataFrame) -> None:\n",
    "    \"\"\"Generate a data profiling report for the master dataset.\"\"\"\n",
    "    logger.info(\"Starting the data profiling process.\")\n",
    "    try:\n",
    "        profile = ProfileReport(dataset, title=\"Data Profiling Report\", explorative=True)\n",
    "        profile.to_file(f\"data_profiling_report_{datetime.now()}.html\")\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error generating data profiling report: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c056a0e",
   "metadata": {},
   "source": [
    "## Data Quality Check (Using Great Expectations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5a4af4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context() -> Any:\n",
    "    \"\"\"Get the context for the Great Expectations data validation.\"\"\"\n",
    "    context = gx.get_context()\n",
    "    return context\n",
    "\n",
    "def gx_expectations_definitions(dataset: pd.DataFrame) -> list:\n",
    "    \"\"\"Define the expectations suite for data validation.\"\"\"\n",
    "    ## Setting data validation rules for each column in the dataset\n",
    "\n",
    "    ## Total Number of rows in the dataset to be checked\n",
    "\n",
    "    dataset.month = dataset.month.astype(str)\n",
    "\n",
    "    dataset_row_number_checker = gx.expectations.ExpectTableRowCountToEqual(\n",
    "        value=dataset.shape[0], severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    dataset_column_number_checker = gx.expectations.ExpectTableColumnCountToEqual(\n",
    "        value=dataset.shape[1], severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    ## Month is to be checked for content to be not null, type datetime and content format YYYY-MM.\n",
    "\n",
    "    month_null_checker = gx.expectations.ExpectColumnValuesToNotBeNull(\n",
    "        column=\"month\", severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    month_type_checker = gx.expectations.ExpectColumnValuesToBeOfType(\n",
    "        column=\"month\", type_=\"str\", severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    month_content_checker = gx.expectations.ExpectColumnValuesToMatchRegex(\n",
    "        column=\"month\", regex=r\"\\d{4}-\\d{2}\", severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    # month is to be checked for the statistical distribution of the values to be between January 2012 and December 2016,\n",
    "    # to prevent data drift in the future when new data is added to the dataset, we can set a baseline distribution for the month column \n",
    "    # using the existing data. This can be done by calculating the histogram of the month values and using it as a reference for \n",
    "    # future checks.\n",
    "\n",
    "    # build a baseline partition from the raw counts instead of the normalized\n",
    "    # frequencies so that `ExpectColumnKLDivergenceToBeLessThan` can consume\n",
    "    # “counts” directly (the expectation will normalise internally).\n",
    "\n",
    "    month_partition_object = {\n",
    "        \"weights\": [float(dataset.month.value_counts().get(month, 0)/dataset.shape[0]) for month in dataset.month.unique().tolist()],\n",
    "        \"values\": dataset.month.astype(str).unique().tolist()\n",
    "    }\n",
    "\n",
    "    month_data_drift_checker = gx.expectations.ExpectColumnKLDivergenceToBeLessThan(\n",
    "        column=\"month\",\n",
    "        partition_object=month_partition_object,\n",
    "        threshold=0.1  # Sensitivity: lower is stricter\n",
    "    )\n",
    "\n",
    "    ## Town is to be checked for existing categories, type to be str and not null.\n",
    "\n",
    "    town_null_checker = gx.expectations.ExpectColumnValuesToNotBeNull(\n",
    "        column=\"town\", severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    town_type_checker = gx.expectations.ExpectColumnValuesToBeOfType(\n",
    "        column=\"town\", type_=\"CategoricalDtypeType\", severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    town_category_checker = gx.expectations.ExpectColumnValuesToBeInSet(\n",
    "        column=\"town\",\n",
    "        value_set=[\n",
    "            \"ANG MO KIO\",\n",
    "            \"BEDOK\",\n",
    "            \"BISHAN\",\n",
    "            \"BUKIT BATOK\",\n",
    "            \"BUKIT MERAH\",\n",
    "            \"BUKIT TIMAH\",\n",
    "            \"BUKIT PANJANG\",\n",
    "            \"CENTRAL AREA\",\n",
    "            \"CHOA CHU KANG\",\n",
    "            \"CLEMENTI\",\n",
    "            \"GEYLANG\",\n",
    "            \"HOUGANG\",\n",
    "            \"JURONG EAST\",\n",
    "            \"JURONG WEST\",\n",
    "            \"KALLANG/WHAMPOA\",\n",
    "            \"MARINE PARADE\",\n",
    "            \"PASIR RIS\",\n",
    "            \"PUNGGOL\",\n",
    "            \"QUEENSTOWN\",\n",
    "            \"SEMBAWANG\",\n",
    "            \"SENGKANG\",\n",
    "            \"SERANGOON\",\n",
    "            \"TAMPINES\",\n",
    "            \"TOA PAYOH\",\n",
    "            \"WOODLANDS\",\n",
    "            \"YISHUN\"\n",
    "        ],\n",
    "        severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    # Checks for data drift for Town column. Same Reasoning as Month column. \n",
    "    # We can set a baseline distribution for the town column using the existing data, \n",
    "    # and use it as a reference for future checks.\n",
    "\n",
    "    town_partition_object = {\n",
    "        \"weights\": [float(dataset.town.value_counts().get(town, 0)/dataset.shape[0]) for town in dataset.town.cat.categories],\n",
    "        \"values\": dataset.town.cat.categories.astype(str).tolist()\n",
    "    }\n",
    "\n",
    "    town_data_drift_checker = gx.expectations.ExpectColumnKLDivergenceToBeLessThan(\n",
    "        column=\"town\",\n",
    "        partition_object=town_partition_object,\n",
    "        threshold=0.1  # Sensitivity: lower is stricter\n",
    "    )\n",
    "\n",
    "    ## Flat Type to be checked …\n",
    "\n",
    "    flat_type_null_checker = gx.expectations.ExpectColumnValuesToNotBeNull(\n",
    "        column=\"flat_type\", severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    flat_type_type_checker = gx.expectations.ExpectColumnValuesToBeOfType(\n",
    "        column=\"flat_type\", type_=\"CategoricalDtypeType\", severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    flat_type_category_checker = gx.expectations.ExpectColumnValuesToBeInSet(\n",
    "        column=\"flat_type\",\n",
    "        value_set=[\n",
    "            \"1 ROOM\",\n",
    "            \"2 ROOM\",\n",
    "            \"3 ROOM\",\n",
    "            \"4 ROOM\",\n",
    "            \"5 ROOM\",\n",
    "            \"EXECUTIVE\",\n",
    "            \"MULTI-GENERATION\"\n",
    "        ],\n",
    "        severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    # Checks for data drift for Flat Type column. Same Reasoning as Month column. \n",
    "    # We can set a baseline distribution for the flat_type column using the existing data, \n",
    "    # and use it as a reference for future checks.\n",
    "\n",
    "    flat_type_partition_object = {\n",
    "        \"weights\": [float(dataset.flat_type.value_counts().get(flat_type, 0)/dataset.shape[0]) for flat_type in dataset.flat_type.cat.categories],\n",
    "        \"values\": dataset.flat_type.cat.categories.astype(str).tolist()\n",
    "    }\n",
    "\n",
    "    flat_type_data_drift_checker = gx.expectations.ExpectColumnKLDivergenceToBeLessThan(\n",
    "        column=\"flat_type\",\n",
    "        partition_object=flat_type_partition_object,\n",
    "        threshold=0.1  # Sensitivity: lower is stricter\n",
    "    )\n",
    "\n",
    "    ## Block Column …\n",
    "\n",
    "    block_null_checker = gx.expectations.ExpectColumnValuesToNotBeNull(\n",
    "        column=\"block\", severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    block_type_checker = gx.expectations.ExpectColumnValuesToBeOfType(\n",
    "        column=\"block\", type_=\"str\", severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    block_format_checker = gx.expectations.ExpectColumnValuesToMatchRegex(\n",
    "        column=\"block\", regex=r\"^[A-Za-z0-9]{1,4}$\", severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    ## Street Name …\n",
    "\n",
    "    street_name_null_checker = gx.expectations.ExpectColumnValuesToNotBeNull(\n",
    "        column=\"street_name\", severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    street_name_type_checker = gx.expectations.ExpectColumnValuesToBeOfType(\n",
    "        column=\"street_name\", type_=\"str\", severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    street_name_no_purely_digit_checker = gx.expectations.ExpectColumnValuesToNotMatchRegex(\n",
    "        column=\"street_name\", regex=r\"^\\d+$\", severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    ## storey_range …\n",
    "\n",
    "    storey_range_null_checker = gx.expectations.ExpectColumnValuesToNotBeNull(\n",
    "        column=\"storey_range\", severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    storey_range_type_checker = gx.expectations.ExpectColumnValuesToBeOfType(\n",
    "        column=\"storey_range\", type_=\"CategoricalDtypeType\", severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    storey_range_format_checker = gx.expectations.ExpectColumnValuesToMatchRegex(\n",
    "        column=\"storey_range\", regex=r\"^\\d{1,2} TO \\d{1,2}$\", severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    # Checks for data drift for Storey Range column. Same Reasoning as Month column. \n",
    "    # We can set a baseline distribution for the storey_range column using the existing data, \n",
    "    # and use it as a reference for future checks.\n",
    "\n",
    "    storey_range_partition_object = {\n",
    "        \"weights\": [float(dataset.storey_range.value_counts().get(storey_range, 0)/dataset.shape[0]) for storey_range in dataset.storey_range.cat.categories],\n",
    "        \"values\": dataset.storey_range.cat.categories.astype(str).tolist()\n",
    "    }\n",
    "\n",
    "    storey_range_data_drift_checker = gx.expectations.ExpectColumnKLDivergenceToBeLessThan(\n",
    "        column=\"storey_range\",\n",
    "        partition_object=storey_range_partition_object,\n",
    "        threshold=0.1  # Sensitivity: lower is stricter\n",
    "    )\n",
    "\n",
    "    ## floor_area_sqm …\n",
    "\n",
    "    floor_area_sqm_null_checker = gx.expectations.ExpectColumnValuesToNotBeNull(\n",
    "        column=\"floor_area_sqm\", severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    floor_area_sqm_type_checker = gx.expectations.ExpectColumnValuesToBeOfType(\n",
    "        column=\"floor_area_sqm\", type_=\"float\", severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    floor_area_sqm_range_checker = gx.expectations.ExpectColumnValuesToBeBetween(\n",
    "        column=\"floor_area_sqm\",\n",
    "        min_value=min(dataset.floor_area_sqm.values),\n",
    "        max_value=max(dataset.floor_area_sqm.values),\n",
    "        severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    ## Flat Model …\n",
    "\n",
    "    flat_model_null_checker = gx.expectations.ExpectColumnValuesToNotBeNull(\n",
    "        column=\"flat_model\", severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    flat_model_type_checker = gx.expectations.ExpectColumnValuesToBeOfType(\n",
    "        column=\"flat_model\", type_=\"CategoricalDtypeType\", severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    flat_model_category_checker = gx.expectations.ExpectColumnValuesToBeInSet(\n",
    "        column=\"flat_model\",\n",
    "        value_set=[\n",
    "            'Improved', 'New Generation', 'Model A', 'Standard', 'Apartment',\n",
    "            'Simplified', 'Model A-Maisonette', 'Maisonette',\n",
    "            'Multi Generation', 'Adjoined flat', 'Premium Apartment',\n",
    "            'Terrace', 'Improved-Maisonette', 'Premium Maisonette', '2-room',\n",
    "            'Model A2', 'DBSS', 'Type S1', 'Type S2', 'Premium Apartment Loft'\n",
    "        ],\n",
    "        severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    # Checks for data drift for Flat Model column. Same Reasoning as Month column. \n",
    "    # We can set a baseline distribution for the flat_model column using the existing data, \n",
    "    # and use it as a reference for future checks.\n",
    "\n",
    "    flat_model_partition_object = {\n",
    "        \"weights\": [float(dataset.flat_model.value_counts().get(flat_model, 0)/dataset.shape[0]) for flat_model in dataset.flat_model.cat.categories],\n",
    "        \"values\": dataset.flat_model.cat.categories.astype(str).tolist()\n",
    "    }\n",
    "\n",
    "    flat_model_data_drift_checker = gx.expectations.ExpectColumnKLDivergenceToBeLessThan(\n",
    "        column=\"flat_model\",\n",
    "        partition_object=flat_model_partition_object,\n",
    "        threshold=0.1  # Sensitivity: lower is stricter\n",
    "    )\n",
    "\n",
    "    ## lease_commence_date …\n",
    "\n",
    "    lease_commence_date_null_checker = gx.expectations.ExpectColumnValuesToNotBeNull(\n",
    "        column=\"lease_commence_date\", severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    lease_commence_date_type_checker = gx.expectations.ExpectColumnValuesToBeOfType(\n",
    "        column=\"lease_commence_date\", type_=\"int\", severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    lease_commence_date_format_checker = gx.expectations.ExpectColumnValuesToMatchRegex(\n",
    "        column=\"lease_commence_date\", regex=r\"^\\d{4}$\", severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    ## remaining_lease …\n",
    "\n",
    "    remaining_lease_range_checker = gx.expectations.ExpectColumnValuesToBeBetween(\n",
    "        column=\"remaining_lease\",\n",
    "        min_value=min(dataset[dataset.remaining_lease.notna()].remaining_lease.values),\n",
    "        max_value=max(dataset[dataset.remaining_lease.notna()].remaining_lease.values),\n",
    "        severity=\"warning\"\n",
    "    )\n",
    "\n",
    "    profilers = [\n",
    "        dataset_row_number_checker,\n",
    "        dataset_column_number_checker,\n",
    "        month_null_checker,\n",
    "        month_type_checker,\n",
    "        month_content_checker,\n",
    "        month_data_drift_checker,\n",
    "        town_null_checker,\n",
    "        town_type_checker,\n",
    "        town_category_checker,\n",
    "        town_data_drift_checker,\n",
    "        flat_type_null_checker,\n",
    "        flat_type_type_checker,\n",
    "        flat_type_category_checker,\n",
    "        flat_type_data_drift_checker,\n",
    "        block_null_checker,\n",
    "        block_type_checker,\n",
    "        block_format_checker,\n",
    "        street_name_null_checker,\n",
    "        street_name_type_checker,\n",
    "        street_name_no_purely_digit_checker,\n",
    "        storey_range_null_checker,\n",
    "        storey_range_type_checker,\n",
    "        storey_range_format_checker,\n",
    "        storey_range_data_drift_checker,\n",
    "        floor_area_sqm_null_checker,\n",
    "        floor_area_sqm_type_checker,\n",
    "        floor_area_sqm_range_checker,\n",
    "        flat_model_null_checker,\n",
    "        flat_model_type_checker,\n",
    "        flat_model_category_checker,\n",
    "        flat_model_data_drift_checker,\n",
    "        lease_commence_date_null_checker,\n",
    "        lease_commence_date_type_checker,\n",
    "        lease_commence_date_format_checker,\n",
    "        remaining_lease_range_checker\n",
    "    ]\n",
    "\n",
    "    return profilers\n",
    "\n",
    "def create_gx_expectations_suite_and_execute(dataset: pd.DataFrame) -> Tuple[bool, list]:\n",
    "    \"\"\"Create the Great Expectations suite and add the defined expectations.\"\"\"\n",
    "    try:\n",
    "\n",
    "        suite = gx.ExpectationSuite(name=\"Expectation_Suite_for_Resale_Flat_Prices_Dataset\")\n",
    "\n",
    "        context = get_context()\n",
    "\n",
    "        data_source = context.data_sources.add_pandas(name = \"Resale_Flat_Prices_Dataset\")\n",
    "        data_asset = data_source.add_dataframe_asset(name=\"data_frame_asset\")\n",
    "\n",
    "        batch_definition = data_asset.add_batch_definition_whole_dataframe(\"Resale_Master_Dataset_Batch_Definition\")\n",
    "        batch_parameters={\"dataframe\": dataset}\n",
    "\n",
    "        profilers = gx_expectations_definitions(dataset)\n",
    "\n",
    "        for checker in profilers:\n",
    "            suite.add_expectation(expectation=checker.copy())\n",
    "\n",
    "        context.suites.add(suite)\n",
    "\n",
    "        definition_name = \"my_validation_definition\"\n",
    "        validation_definition = gx.ValidationDefinition(\n",
    "            data=batch_definition, suite=suite, name=definition_name\n",
    "        )\n",
    "\n",
    "        # Run the validation checks\n",
    "        validation_definition.run(batch_parameters=batch_parameters)\n",
    "\n",
    "        # Retrieve the validation definition\n",
    "        validation_definition = context.validation_definitions.get(\"my_validation_definition\")\n",
    "\n",
    "        # Create a Checkpoint\n",
    "        checkpoint_name = \"Data Validation Checks\"\n",
    "        checkpoint_config = gx.Checkpoint(\n",
    "            name=checkpoint_name, validation_definitions=[validation_definition], result_format=ResultFormat.COMPLETE\n",
    "        )\n",
    "\n",
    "        # Save the Checkpoint to the data context\n",
    "        checkpoint = context.checkpoints.add(checkpoint_config)\n",
    "\n",
    "        # Run the Checkpoint\n",
    "        result = checkpoint.run(batch_parameters=batch_parameters)\n",
    "\n",
    "        main_identifier = None\n",
    "\n",
    "        for key, _ in result.run_results.items():\n",
    "            if main_identifier is None:\n",
    "                main_identifier = key\n",
    "\n",
    "        logger.info(f\"This is the main identifier {main_identifier}\")\n",
    "\n",
    "        all_results = result.run_results.get(main_identifier)\n",
    "\n",
    "        if not all_results.get(\"success\"):\n",
    "            logger.info(\"Validation failed!\")\n",
    "            output_list = write_to_text_file_failure_records_and_retrieve_index(validation_results=all_results)\n",
    "            return (True, output_list)\n",
    "\n",
    "        logger.info(f\"Success in {len(profilers)} Data Validation. Writing the results to a file\")\n",
    "        write_to_text_file_success_records(validation_results=all_results)\n",
    "        return (False, [])\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Data Validation Check Failed. Error - {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895b7c37",
   "metadata": {},
   "source": [
    "## Data Quality Check (Anomalous Records and Check for Duplicate Records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9204cf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lease_computation(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Computation of the remaining lease for the HDB flats in the dataset.\"\"\"\n",
    "    logger.info(\"Computing lease for the dataset.\")\n",
    "    try:\n",
    "        dataset.remaining_lease = dataset.apply(lambda x: 99 - (datetime.now().year - int(x.lease_commence_date)), axis=1)\n",
    "        dataset.remaining_lease = dataset.remaining_lease.astype(str).apply(lambda x: f\"{int(x)//12} years {int(x)%12} months\")\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Lease computation error - {e}\")\n",
    "        raise e\n",
    "\n",
    "def define_composite_key(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Computes the composite key.\"\"\"\n",
    "    logger.info(\"Define composite key\")\n",
    "    try:\n",
    "        dataset['composite_key'] = dataset.apply(lambda x: f\"{x.month.replace('-', '')}_{x.town.replace(' ', '')}_{x.flat_type.replace('-', '').replace(' ', '')}_{x.block}_{x.street_name.replace(' ', '')}_{x.storey_range.replace(' ', '')}_{str(int(x.floor_area_sqm))}_{x.flat_model.replace(' ', '')}_{x.lease_commence_date}_{x.remaining_lease.replace(' ', '')}_{x._id}\", axis=1)\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Define composite key error - {e}\")\n",
    "        raise e\n",
    "\n",
    "def filter_for_anomalous_resale_prices(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Finds the anomalous price based on 3 standard deviations of the distribution\n",
    "    of the dataset, based on grouping of month and town. This is assuming that each\n",
    "    town has its own level of wealth distribution in this country. The anomalous\n",
    "    price can only be computed when the whole raw dataset has been compiled. Grouping\n",
    "    of the month is done as I am assuming that different months will see different\n",
    "    price trend.\"\"\"\n",
    "    anomalous_flag = dataset.groupby(['month', 'town']).apply(lambda x: x.resale_price.mean() + 3*x.resale_price.std() < x.resale_price).reset_index(name='is_anomalous')\n",
    "    anomalous_records = dataset[dataset.index.isin(anomalous_flag[anomalous_flag['is_anomalous'] == True].level_2.tolist())]\n",
    "    normal_records = dataset[dataset.index.isin(anomalous_flag[anomalous_flag['is_anomalous'] == False].level_2.tolist())]\n",
    "    return (normal_records, anomalous_records)\n",
    "\n",
    "def manual_data_quality_checks_and_fixes(dataset: pd.DataFrame) -> None:\n",
    "    \"\"\"Data transformation operations for the data pipeline.\"\"\"\n",
    "    logger.info(\"Manual data quality checks.\")\n",
    "    if dataset.shape[0] == 0:\n",
    "        return (None, None, None)\n",
    "    # First Step: Compute the remaining lease.\n",
    "    lease_computation(dataset=dataset)\n",
    "    # Second Step: Define the composite key for setting the primary key of the dataset.\n",
    "    define_composite_key(dataset=dataset)\n",
    "    subset_unique_records, subset_duplicate_records = remove_duplicates_based_on_fields(fields=['composite_key', 'resale_price'], dataset=dataset)\n",
    "    normal_records, anomalous_records = filter_for_anomalous_resale_prices(dataset=subset_unique_records)\n",
    "    return (normal_records, anomalous_records, subset_duplicate_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c3fde7",
   "metadata": {},
   "source": [
    "## Data Manipulation (Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f18cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_resale_identifier(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create the resale identifier column\"\"\"\n",
    "    try:\n",
    "        first_resale_identifier_column = [\"S\"] * dataset.shape[0]\n",
    "        second_resale_identifier_column = dataset.block.apply(lambda x: x[:3] if len(x) >= 3 else x.zfill(3))\n",
    "        third_resale_identifier_average_price = dataset.groupby(['month', 'town', 'flat_type']).agg({'resale_price': 'mean'}).reset_index()\n",
    "        third_resale_identifier_average_price.resale_price = third_resale_identifier_average_price.resale_price.apply(lambda x: str(x)[0:2])\n",
    "        third_resale_identifier_column = dataset.merge(third_resale_identifier_average_price, on=['month', 'town', 'flat_type'], how='left', suffixes=('', '_avg')).resale_price_avg\n",
    "        fourth_resale_identifier_column = dataset.month.apply(lambda x: str(x)[len(str(x))-2:])\n",
    "        fifth_resale_identifier_column = dataset.town.apply(lambda x: x[0])\n",
    "        resale_identifier_column = pd.DataFrame()\n",
    "        resale_identifier_column['first'] = first_resale_identifier_column\n",
    "        resale_identifier_column['second'] = second_resale_identifier_column.tolist()\n",
    "        resale_identifier_column['third'] = third_resale_identifier_column.tolist()\n",
    "        resale_identifier_column['fourth'] = fourth_resale_identifier_column.tolist()\n",
    "        resale_identifier_column['fifth'] = fifth_resale_identifier_column.tolist()\n",
    "        resale_identifier_values = resale_identifier_column.apply(lambda x: ''.join(x.astype(str)), axis=1)\n",
    "        dataset[\"resale_identifier\"] = resale_identifier_values.tolist()\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error in creating resale identifier. Error - {e}\")\n",
    "        raise e\n",
    "\n",
    "def irreversible_hash_sha255(resale_identifier: str) -> str:\n",
    "    \"\"\"Hashes the resale identifier column. SHA 256 is chosen as it is much\n",
    "    cryptographically stronger.\"\"\"\n",
    "    try:\n",
    "        # Ensure the input data is encoded to bytes\n",
    "        encoded_data = resale_identifier.encode('utf-8')\n",
    "        # Create a new SHA256 hash object\n",
    "        hash_object = hashlib.sha256(encoded_data)\n",
    "        # Get the hexadecimal representation of the hash\n",
    "        hex_digest = hash_object.hexdigest()\n",
    "        return hex_digest\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error in creating hash for resale identifier. Error - {e}\")\n",
    "        raise e\n",
    "\n",
    "def data_transformation_operations(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Data Transformation Process\"\"\"\n",
    "    resale_identifier_dataset = creating_resale_identifier(dataset=dataset)\n",
    "    subset_unique_records, subset_duplicate_records = remove_duplicates_based_on_fields(fields=['resale_identifier', 'resale_price'], dataset=resale_identifier_dataset)\n",
    "    hashed_data = subset_unique_records.copy()\n",
    "    hashed_data['resale_identifier_hashed'] = hashed_data['resale_identifier'].apply(lambda x: irreversible_hash_sha255(x))\n",
    "    return (resale_identifier_dataset, subset_duplicate_records, hashed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919fec02",
   "metadata": {},
   "source": [
    "## Storage (Load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76a2e844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets_on_local_storage(raw: pd.DataFrame, clean: pd.DataFrame, transformed: pd.DataFrame, failed: pd.DataFrame, hashed: pd.DataFrame) -> None:\n",
    "    \"\"\"Writes out the produced output files as per the needs of the Technical Assessment.\"\"\"\n",
    "    if \"output_files\" not in os.listdir():\n",
    "        logger.info(\"Creating the folder 'output_files' for the first time.\")\n",
    "        os.mkdir(\"output_files\")\n",
    "    logger.info(\"Proceeds to write out the five requested data artifacts: Raw, Cleaned, Transformed, Failed and Hashed.\")\n",
    "    write_to_csv(dataset=raw, filename=\"./output_files/raw.csv\")\n",
    "    write_to_csv(dataset=clean, filename=\"./output_files/clean.csv\")\n",
    "    write_to_csv(dataset=transformed, filename=\"./output_files/transformed.csv\")\n",
    "    write_to_csv(dataset=failed, filename=\"./output_files/failed.csv\")\n",
    "    write_to_csv(dataset=hashed, filename=\"./output_files/hashed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d5e761",
   "metadata": {},
   "source": [
    "## Automated ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4fcc066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def automated_etl_flow():\n",
    "    logger.info(\"Starting the ETL flow. First step: downloading datasets if the datasets do not already exist.\")\n",
    "    # Check for all files in the current directory. Please rerun the constants segment to check for existing files,\n",
    "    # as the jupyter notebook is unidirectional in execution.\n",
    "    if not all([file in ALL_FILES for file in NEEDED_FILES]):\n",
    "        logger.info(\"Not all datasets found in the current directory. Initiating download process.\")\n",
    "        source_download_main()\n",
    "    logger.info(\"All datasets are present. ETL flow complete.\")\n",
    "    logger.info(\"Next steps would be to profile the data. Second Step would be to read in the datasets and create a profile report for each dataset.\")\n",
    "    raw_dataset, complete_dataset = load_up_dataset()\n",
    "    logger.info(\"Data loading complete. Third step is data profiling and validation.\")\n",
    "    data_profiling(complete_dataset)\n",
    "    logger.info(\"Data profiling complete. Fourth step is to create Great Expectations suite and execute it against the dataset.\")\n",
    "    validation_failure_status, output = create_gx_expectations_suite_and_execute(dataset=complete_dataset)\n",
    "    validated_records, failed_validation_records = filter_for_failed_validation_records(validation_failure_status=validation_failure_status, output=output, dataset=complete_dataset)\n",
    "    normal_records, failed_records, interim_duplicated_records = manual_data_quality_checks_and_fixes(dataset=validated_records)\n",
    "    if normal_records is not None:\n",
    "        logger.info(\"Data Transformation completed. Working on data validation to check the data again.\")\n",
    "        processed_dataset, final_duplicated_records, hashed_data = data_transformation_operations(dataset=normal_records)\n",
    "        failed_records = append_datasets_for_storage(datasets=[interim_duplicated_records, final_duplicated_records, failed_records, failed_validation_records])\n",
    "        logger.info(\"Writes out the output files into the folder 'output_files'\")\n",
    "        datasets_on_local_storage(raw=raw_dataset, clean=normal_records, transformed=processed_dataset, failed=failed_records, hashed=hashed_data)\n",
    "    else:\n",
    "        datasets_on_local_storage(raw=raw_dataset, clean=pd.DataFrame(), transformed=pd.DataFrame(), failed=failed_validation_records, hashed=pd.DataFrame())\n",
    "    logger.info(\"ETL Process has come to an end.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8689242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-22 11:02:38.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mautomated_etl_flow\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mStarting the ETL flow. First step: downloading datasets if the datasets do not already exist.\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:38.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mautomated_etl_flow\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mAll datasets are present. ETL flow complete.\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:38.054\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mautomated_etl_flow\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mNext steps would be to profile the data. Second Step would be to read in the datasets and create a profile report for each dataset.\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:38.054\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_up_dataset\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mStarting the dataset loading process.\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:38.054\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mread_dataset\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mRead in the three datasets that ranges from January 2000 to December 2016.\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:38.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_up_dataset\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mNumber of rows and columns for Full Raw Dataset: (459007, 12)\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:38.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_up_dataset\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mEnforcing data types for the master dataset.\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:38.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdataset_type_enforcement\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mConduct type enforcement on the fields.\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:38.461\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_up_dataset\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mFiltering the master dataset to only include records from January 2012 to December 2016.\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:38.461\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfilter_2012_to_2016\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mFilter the dataset to contain between January 2012 to December 2016.\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:38.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_up_dataset\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mNumber of rows and columns for Filtered Dataset: (92544, 12)\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:38.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mautomated_etl_flow\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mData loading complete. Third step is data profiling and validation.\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:38.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdata_profiling\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mStarting the data profiling process.\u001b[0m\n",
      "/Users/wongyokeyong/Desktop/Resume/Public Sector Tech Career Fair/HDBTechnicalTest/.hdbtechnicaltest/lib/python3.10/site-packages/ydata_profiling/utils/dataframe.py:137: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns={\"index\": \"df_index\"}, inplace=True)\n",
      "100%|██████████| 12/12 [00:00<00:00, 917.92it/s]00:00, 21.59it/s, Describe variable: remaining_lease]\n",
      "Summarize dataset: 100%|██████████| 46/46 [00:01<00:00, 23.45it/s, Completed]                                       \n",
      "Generate report structure: 100%|██████████| 1/1 [00:01<00:00,  1.91s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:00<00:00, 13.02it/s]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 438.83it/s]\n",
      "\u001b[32m2026-02-22 11:02:42.955\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mautomated_etl_flow\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mData profiling complete. Fourth step is to create Great Expectations suite and execute it against the dataset.\u001b[0m\n",
      "/var/folders/9x/p_72xh6s3zv8xdfqxc_km__40000gn/T/ipykernel_6508/838305622.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset.month = dataset.month.astype(str)\n",
      "Calculating Metrics: 100%|██████████| 154/154 [00:00<00:00, 339.73it/s]\n",
      "Calculating Metrics: 100%|██████████| 154/154 [00:00<00:00, 336.24it/s]\n",
      "\u001b[32m2026-02-22 11:02:44.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcreate_gx_expectations_suite_and_execute\u001b[0m:\u001b[36m377\u001b[0m - \u001b[1mThis is the main identifier ValidationResultIdentifier::Expectation_Suite_for_Resale_Flat_Prices_Dataset/__none__/20260222T030243.993143Z/Resale_Flat_Prices_Dataset-data_frame_asset\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:44.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcreate_gx_expectations_suite_and_execute\u001b[0m:\u001b[36m386\u001b[0m - \u001b[1mSuccess in 35 Data Validation. Writing the results to a file\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:44.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mwrite_to_text_file_success_records\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mWritten the successful validation definition to a text file for checking purposes\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:44.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmanual_data_quality_checks_and_fixes\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mManual data quality checks.\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:44.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlease_computation\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mComputing lease for the dataset.\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:45.125\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdefine_composite_key\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mDefine composite key\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:47.978\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mremove_duplicates_based_on_fields\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mRemove duplicates based on the fields supplied for sorting first, then to remove duplicates.\u001b[0m\n",
      "/var/folders/9x/p_72xh6s3zv8xdfqxc_km__40000gn/T/ipykernel_6508/3618418940.py:29: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  anomalous_flag = dataset.groupby(['month', 'town']).apply(lambda x: x.resale_price.mean() + 3*x.resale_price.std() < x.resale_price).reset_index(name='is_anomalous')\n",
      "/var/folders/9x/p_72xh6s3zv8xdfqxc_km__40000gn/T/ipykernel_6508/3618418940.py:29: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  anomalous_flag = dataset.groupby(['month', 'town']).apply(lambda x: x.resale_price.mean() + 3*x.resale_price.std() < x.resale_price).reset_index(name='is_anomalous')\n",
      "\u001b[32m2026-02-22 11:02:48.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mautomated_etl_flow\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mData Transformation completed. Working on data validation to check the data again.\u001b[0m\n",
      "/var/folders/9x/p_72xh6s3zv8xdfqxc_km__40000gn/T/ipykernel_6508/3321048278.py:6: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  third_resale_identifier_average_price = dataset.groupby(['month', 'town', 'flat_type']).agg({'resale_price': 'mean'}).reset_index()\n",
      "\u001b[32m2026-02-22 11:02:50.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mremove_duplicates_based_on_fields\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mRemove duplicates based on the fields supplied for sorting first, then to remove duplicates.\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:50.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mautomated_etl_flow\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mWrites out the output files into the folder 'output_files'\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:50.873\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdatasets_on_local_storage\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mProceeds to write out the five requested data artifacts: Raw, Cleaned, Transformed, Failed and Hashed.\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:52.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mwrite_to_csv\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mSuccessfully wrote DataFrame to CSV file: ./output_files/raw.csv\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:52.558\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mwrite_to_csv\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mSuccessfully wrote DataFrame to CSV file: ./output_files/clean.csv\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:52.926\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mwrite_to_csv\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mSuccessfully wrote DataFrame to CSV file: ./output_files/transformed.csv\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:52.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mwrite_to_csv\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mSuccessfully wrote DataFrame to CSV file: ./output_files/failed.csv\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:53.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mwrite_to_csv\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mSuccessfully wrote DataFrame to CSV file: ./output_files/hashed.csv\u001b[0m\n",
      "\u001b[32m2026-02-22 11:02:53.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mautomated_etl_flow\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mETL Process has come to an end.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "automated_etl_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4e0e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".hdbtechnicaltest (3.10.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
